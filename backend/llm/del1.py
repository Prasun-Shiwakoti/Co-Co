import json

print(json.dumps({"text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.Recurrent neural networks, long short-term memory and gated recurrent neural networks have been firmly established as state of the art approaches in sequence modeling and transduction problems. Niki designed, implemented, tuned and evaluated countless model  and was involved in nearly every detail of this work.Llion, Lukasz and Aidan spent countless long days designing various parts of and  implementing tensor2tensor. variants in our original codebase. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations.Attention mechanisms have become an integral part of compelling sequence modeling and transduction models. In all but a few cases such attention mechanisms are used in conjunction with a recurrent network. We propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in computer science.The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions. This makes it more difficult to learn dependencies between distant positions. In the Transformer this is reduced to a constant number of Operations.Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations. The Transformer is the first transduction model relying entirely on self-att attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-Attention and discuss its advantages over models such as [14] [15] and [8].\"Multi-Head Attention consists of several attention layers running in parallel. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. Dot-product attention is identical to our algorithm, except for the scaling factor of 1/√dk. additive attention outperforms dot product attention without scaling for larger values of dk. Multi-head attention allows the model to jointly attend to infor. ix multiplication code. We suspect that for large values of. dk, the dot products grow large in magnitude, pushing the softmax. function into regions where it has extremely small gradients.With a single attention head, averaging inhibits this. To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q · k = ∑dki=1 qi ki, has mean 0, and variance dk. This is because the mean value of the product of two independentrandom variables is 0.The Transformer uses multi-head attention in three different ways. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. In the decoder, each position can attend to all positions in the previous layers. The reduced dimension of each head means that the total computational cost is similar to that of single-head Attention.Each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out all values in the input of the softmax.In order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers.We consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long- range dependencies is a key challenge in many sequence transduction tasks.A self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires $O(n)$ sequential operations. In terms of computational complexity, self-Attention layers are faster than recurrent layers when the sequence length $n$ is smaller than the representation dimensionality $d$In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations. We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies. A self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires $O(n)$ sequential operations.Self-attention layers are faster than recurrent layers when the sequence length $n$ is smaller than the representation dimensionality $d$ This is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece and byte-pair representations. We plan to investigate this approach further in future work.Kernel width $k<n$ does not connect all pairs of input and output positions. Convolutional layers are generally more expensive than recurrent layers, by a factor of $k$. Separable convolutions $[6]$, however, decrease the complexity considerably. We inspect attention distributions from our models and present and discuss examples.This section describes the training regime for our models. We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding $[3]$, which has a shared source-target vocabulary of about 37000 tokens.The big models were trained for 300,000 steps (3.5 days) We used $warmup\_steps = 4000$. We employ three types of regularization during training. We apply dropout to the output of each sub-layer, before it is added to the sub- layer input and normalized.E sums of the embeddings and the positional encodings in both the encoder and decoder stacks are used. For the base model, we use a rate of $P_{drop}=0.1$ for the BLEU-4 score.Our approach consists of three stages: pre-computation, model modification, and fine-tuning. We compute the shape and size embedding directly, but also construct six sets of shape andsize features for each shape. In the model modification phase, we set parameters of shape. and size. embeddings to values that are obtained in the pre.computed phase. We then fine-tune the shape, size, and test data.A major experiment consists of eight models being trained. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding.Table 3 rows (B), we observe that reducing the attention key size $d_k$ hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger m. bigger m is better.In this work, we presented the Transformer, the first sequence transduction model based entirely on attention. The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. In the former task our best model outperforms even all previously reported ensembles.\"Making generation less sequential is another research goals of ours,\" says Hinton. \"To investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. ext and to investigate local,. restricted attention mechanism to efficiently handling large. inputs and Outputs such as image and video"}))